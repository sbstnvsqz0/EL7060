{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"CREMA-D\"\n",
    "SAMPLERATE = 16000\n",
    "FRAME_SIZE = 1280\n",
    "HOP = 256\n",
    "N_MELS = 28\n",
    "N_FFT = 1280\n",
    "N_MFCC = 13 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CremaDDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dir, preprocessing=None):\n",
    "        assert dir in [\"train\",\"validation\",\"test\"], \"Conjunto invalido\"\n",
    "        self.dir = dir\n",
    "        self.df = (pd.read_csv(os.path.join(DATA_FOLDER, 'labels.csv'))[lambda x: x['partition'] == f\"{self.dir}\"])\n",
    "        self.preprocessing = preprocessing \n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        signal_path = os.path.join(DATA_FOLDER,self.df.iloc[idx][\"path\"])\n",
    "        waveform,_ = librosa.load(signal_path,sr=SAMPLERATE)\n",
    "        label =  self.df.iloc[idx][\"class\"]\n",
    "        if self.preprocessing is not None:\n",
    "            not_padded_features = self.preprocessing.transform(waveform,pad=False)\n",
    "            features = self.preprocessing.transform(waveform)\n",
    "        else: \n",
    "            features = waveform\n",
    "        \n",
    "        return features,label, not_padded_features.size(0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.functional as F\n",
    "import torch\n",
    "from src.utils.utils import get_MFCC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \"\"\"\n",
    "    Salen (n_windows,n_MFCC)\n",
    "    \"\"\"\n",
    "    def __init__(self, frame_size, hop, n_mels, n_fft, n_mfcc, samplerate):\n",
    "        self.samplerate=samplerate\n",
    "        self.frame_size = frame_size\n",
    "        self.hop = hop\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        \n",
    "\n",
    "        self.mfcc = get_MFCC(frame_size, \n",
    "                     hop, \n",
    "                     n_mels, \n",
    "                     n_fft,\n",
    "                     n_mfcc, \n",
    "                     samplerate)\n",
    "    \n",
    "    def transform(self,waveform,pad=True):\n",
    "        signal = torch.Tensor(waveform)\n",
    "        features = self.mfcc(signal).T\n",
    "        # Padding (no se procesa)\n",
    "        if pad:\n",
    "            zeros = torch.zeros(500-features.size()[0],features.size()[1])\n",
    "                   \n",
    "            features = torch.cat((features,zeros),dim = 0)  \n",
    "        return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Preprocessing(FRAME_SIZE, HOP, N_MELS, N_FFT, N_MFCC, SAMPLERATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CremaDDataset(dir = \"train\",preprocessing = p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-3.5713,  3.0809,  2.0625,  ...,  1.4525,  1.8131,  2.0151],\n",
       "         [36.6516, 46.9749, 21.1307,  ..., -0.3892, -1.2355, -1.5876],\n",
       "         [75.6386, 61.2862, 17.3726,  ...,  0.1340, -0.5726, -3.8236],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " 5,\n",
       " 143)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_larges(batch):\n",
    "    list_larges = [batch[0].size()[0] for x in dataset]\n",
    "    return list_larges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "larges = get_larges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([143, 13])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5: 879, 3: 879, 2: 879, 4: 879, 0: 752, 1: 879}\n"
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "for _,label in dataset:\n",
    "    if label not in d.keys():\n",
    "        d[label] = 1\n",
    "    else:\n",
    "        d[label]+=1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "larges = []\n",
    "for signal,_ in dataset:\n",
    "    larges.append(signal.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.array(larges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x10ed87cc580>,\n",
       "  <matplotlib.lines.Line2D at 0x10ed87cc820>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x10ed87ccac0>,\n",
       "  <matplotlib.lines.Line2D at 0x10ed87ccd60>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x10ed87cc2e0>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x10ed87cd000>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x10ed87cd2a0>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(larges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "p = Preprocessing(FRAME_SIZE, HOP, N_MELS, N_FFT, N_MFCC, SAMPLERATE)\n",
    "dataset = CremaDDataset(dir=\"train\",preprocessing=p)\n",
    "dataloader = DataLoader(dataset,batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: MLPCustom\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "class HiddenLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    - hidden_dim: dimensión de entrada y salida de la capa.\n",
    "    - dropout: probabilidad de dropout\n",
    "\n",
    "    output: modulo con capas Linear, ReLU y Dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, dropout):\n",
    "        super(HiddenLayer, self).__init__()\n",
    "        self.layer = nn.Linear(input_dim, output_dim)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x0 = self.layer(x)\n",
    "        x1 = self.activation(x0)\n",
    "        out = self.dropout(x1)\n",
    "        return out\n",
    "    \n",
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim,n_layers,dropout):\n",
    "        super().__init__()\n",
    "        hidden_dims = [input_dim//(2**k) for k in range(1,n_layers)]\n",
    "        self.first = nn.Linear(input_dim,hidden_dims[0])\n",
    "        self.hidden = nn.ModuleList([HiddenLayer(hidden_dims[k],hidden_dims[k+1],dropout) for k in range(len(hidden_dims)-1)]) #Cadena de capas ocultas\n",
    "        self.out = nn.Linear(hidden_dims[-1],output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self,x):\n",
    "        x = self.first(x)\n",
    "        for layer in self.hidden:\n",
    "            x = layer(x)\n",
    "        x = self.out(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, input_size,hidden_size, num_lstm_layers,num_mlp_layers, output_size, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # capa recurrente\n",
    "        self.input_size = input_size\n",
    "        self.lstm = torch.nn.LSTM(input_size=self.input_size, \n",
    "                                  hidden_size=hidden_size, \n",
    "                                  num_layers=num_lstm_layers, \n",
    "                                  batch_first=True,\n",
    "                                  dropout = dropout)\n",
    "\n",
    "        # capa fully conected\n",
    "        self.fc = CustomMLP(input_dim = hidden_size,\n",
    "                            output_dim = output_size,\n",
    "                            n_layers = num_mlp_layers,\n",
    "                            dropout = dropout)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        \n",
    "        packed_input = rnn_utils.pack_padded_sequence(x,lengths,batch_first=True,enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = rnn_utils.pad_packed_sequence(packed_output,batch_first=True)\n",
    "        valid_output = self.get_last_valid_output(output,lengths)\n",
    "        mlp_output = self.fc(valid_output)\n",
    "        return mlp_output\n",
    "    \n",
    "    def get_last_valid_output(self,output,lengths):\n",
    "        batch_size = output.size(0)\n",
    "        last_valid_output = []\n",
    "        for i in range(batch_size):\n",
    "            l = lengths[i]\n",
    "            last_valid_output.append(output[i,l-1,:])\n",
    "        return torch.stack(last_valid_output)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomLSTM(input_size = N_MFCC,\n",
    "                   hidden_size = 128,\n",
    "                   num_lstm_layers = 2,\n",
    "                   num_mlp_layers = 3,\n",
    "                   output_size = 6, \n",
    "                   dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import StepLR, CyclicLR\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "SEED=30\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngineLSTM:\n",
    "    def __init__(self,input_size,hidden_size,num_lstm_layers,num_mlp_layers, output_size, dropout,batch_size,learning_rate,preprocessing):\n",
    "        torch.manual_seed(SEED)\n",
    "        self.model = CustomLSTM(input_size = input_size,\n",
    "                                hidden_size = hidden_size,\n",
    "                                num_lstm_layers = num_lstm_layers,\n",
    "                                num_mlp_layers = num_mlp_layers,\n",
    "                                output_size = output_size, \n",
    "                                dropout=dropout).to(DEVICE)\n",
    "        self.batch_size = batch_size\n",
    "        #self.optimizer = SGD(self.model.parameters(),lr=learning_rate,momentum=0.9) \n",
    "        self.preprocessing = preprocessing\n",
    "        self.optimizer = Adam(self.model.parameters(),lr=learning_rate)\n",
    "        #self.scheduler = CyclicLR(self.optimizer,base_lr=0.1*learning_rate,max_lr=learning_rate,step_size_up=1000)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.best_val_loss = np.inf\n",
    "        self.acc = 0\n",
    "        self.best_acc = 0\n",
    "        \n",
    "    def train(self,epochs,patience,delta,save_model=True):\n",
    "        dataloader = DataLoader(CremaDDataset(\"train\",self.preprocessing),batch_size=self.batch_size,shuffle=True)\n",
    "        dataloader_eval = DataLoader(CremaDDataset(\"validation\",self.preprocessing),shuffle=False)\n",
    "        p = 0\n",
    "        status = 0\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            val_loss = 0\n",
    "            acc = 0 \n",
    "            with tqdm(total=len(dataloader),desc=f'Epoca {epoch}/{epochs}',unit=\"batch\") as pbar:\n",
    "                for batch in dataloader:\n",
    "                    self.optimizer.zero_grad()\n",
    "                    input,label,lengths = batch[0].float().to(DEVICE),batch[1].type(torch.uint8).to(DEVICE), batch[2]\n",
    "                    \n",
    "                    pred = self.model(input,lengths)\n",
    "                    label_pred = torch.argmax(pred,dim=1)\n",
    "\n",
    "                    \n",
    "                    t_loss = self.criterion(pred,label)\n",
    "                    train_loss +=t_loss.item()  #Acumulo losses por época\n",
    "                    t_loss.backward()   #Propago loss en cada batch\n",
    "\n",
    "                    self.optimizer.step()   #Step en cada batch\n",
    "                    #self.scheduler.step()\n",
    "                    pbar.update(1)\n",
    "            train_loss = train_loss/len(dataloader)\n",
    "            self.train_losses.append(train_loss)\n",
    "\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(dataloader_eval, total=len(dataloader_eval),desc=\"Validación\"):\n",
    "                    input,label,lengths = batch[0].float().to(DEVICE),batch[1].type(torch.uint8).to(DEVICE), batch[2]\n",
    "                    pred = self.model(input,lengths)\n",
    "                    label_pred = torch.argmax(pred,dim=1)\n",
    "\n",
    "                    val_loss += self.criterion(pred,label).item()\n",
    "                    acc += (label_pred==label).sum()\n",
    "                \n",
    "                val_loss = val_loss/len(dataloader_eval)\n",
    "                acc = acc/len(dataloader_eval)\n",
    "                self.acc = acc\n",
    "            \n",
    "            \n",
    "            self.val_losses.append(val_loss)\n",
    "\n",
    "            print(\"Epoca: {}, \\tTrain_loss: {:.4f}, \\tVal loss: {:.4f}, \\tAcc: {:.4f}\".format(epoch,train_loss,val_loss,acc))\n",
    "            if acc > self.best_acc:\n",
    "                self.best_acc = acc\n",
    "                if save_model:\n",
    "                    self.save_model()\n",
    "            if val_loss+delta<self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.acc = acc\n",
    "                p = 0\n",
    "            else:\n",
    "                p+=1\n",
    "            \n",
    "            if p==patience:\n",
    "                self.train_losses = self.train_losses[:-patience]\n",
    "                self.val_losses = self.val_losses[:-patience]\n",
    "                status = 1\n",
    "                print(\"Patience alcanzada, terminando entrenamiento\")\n",
    "                break\n",
    "        if status == 0:\n",
    "            print(\"El entrenamiento ha concluido dado que se llegó a las épocas\")\n",
    "\n",
    "    def evaluate(self,dataloader):\n",
    "        self.model.eval()\n",
    "        acc = 0\n",
    "        real_labels = []\n",
    "        pred_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, total=len(dataloader),desc=\"Evaluación\"):\n",
    "                input,label = batch[0].float().to(DEVICE),batch[1].type(torch.uint8).to(DEVICE)\n",
    "                \n",
    "                pred = self.model(input)\n",
    "                label_pred = torch.argmax(pred,dim=1)\n",
    "                real_labels.append(label.cpu())\n",
    "                pred_labels.append(label_pred.cpu())\n",
    "                acc += (label_pred==label).sum()\n",
    "            acc = acc/len(dataloader)\n",
    "            conf_matrix = confusion_matrix(y_true = real_labels,\n",
    "                                             y_pred = pred_labels,\n",
    "                                             normalize='true')\n",
    "            sns.heatmap(conf_matrix,annot=True,cmap=\"summer\")\n",
    "            plt.xlabel(\"Valores predichos\")\n",
    "            plt.ylabel(\"Valores reales\")\n",
    "            plt.show()\n",
    "            print(f\"Accuracy: {acc}\")\n",
    "        \n",
    "    def save_model(self):\n",
    "        torch.save(self.model.state_dict(), \"model.pth\")\n",
    "\n",
    "    def load_model(self,path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "\n",
    "    def return_losses(self):\n",
    "        return [self.train_losses, self.val_losses]\n",
    "    \n",
    "    def return_acc(self):\n",
    "        return self.acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "EngineLSTM.__init__() got an unexpected keyword argument 'input_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[307], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m e \u001b[38;5;241m=\u001b[39m \u001b[43mEngineLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_MFCC\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mD_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: EngineLSTM.__init__() got an unexpected keyword argument 'input_dim'"
     ]
    }
   ],
   "source": [
    "e = EngineLSTM(input_dim=N_MFCC,hidden_dim=128,num_layers=2,D_out=6,batch_size=64,learning_rate=0.01,preprocessing=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = EngineLSTM(input_size = N_MFCC,\n",
    "               hidden_size = 128,\n",
    "               num_lstm_layers = 2,\n",
    "               num_mlp_layers = 3, \n",
    "               output_size = 6, \n",
    "               dropout = 0.2,\n",
    "               batch_size = 64,\n",
    "               learning_rate = 0.001,\n",
    "               preprocessing = p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 0/100: 100%|██████████| 81/81 [00:46<00:00,  1.73batch/s]\n",
      "Validación: 100%|██████████| 1475/1475 [00:32<00:00, 45.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 0, \tTrain_loss: 1.6185, \tVal loss: 1.6180, \tAcc: 0.4115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 1/100: 100%|██████████| 81/81 [00:48<00:00,  1.67batch/s]\n",
      "Validación: 100%|██████████| 1475/1475 [00:27<00:00, 53.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 1, \tTrain_loss: 1.6326, \tVal loss: 1.6154, \tAcc: 0.4102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 2/100: 100%|██████████| 81/81 [00:40<00:00,  2.00batch/s]\n",
      "Validación: 100%|██████████| 1475/1475 [00:29<00:00, 49.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 2, \tTrain_loss: 1.6431, \tVal loss: 1.6380, \tAcc: 0.3993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 3/100: 100%|██████████| 81/81 [00:41<00:00,  1.95batch/s]\n",
      "Validación: 100%|██████████| 1475/1475 [00:28<00:00, 51.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 3, \tTrain_loss: 1.6260, \tVal loss: 1.6108, \tAcc: 0.4210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 4/100:  31%|███       | 25/81 [00:12<00:28,  1.96batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[311], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[306], line 33\u001b[0m, in \u001b[0;36mEngineLSTM.train\u001b[1;34m(self, epochs, patience, delta, save_model)\u001b[0m\n\u001b[0;32m     31\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader),desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoca \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28minput\u001b[39m,label,lengths \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(DEVICE),batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mto(DEVICE), batch[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\sebas\\Desktop\\DL_señales\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\sebas\\Desktop\\DL_señales\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\Desktop\\DL_señales\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\sebas\\Desktop\\DL_señales\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[278], line 13\u001b[0m, in \u001b[0;36mCremaDDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     11\u001b[0m label \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     not_padded_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mtransform(waveform)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n",
      "Cell \u001b[1;32mIn[279], line 22\u001b[0m, in \u001b[0;36mPreprocessing.transform\u001b[1;34m(self, waveform, pad)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m,waveform,pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m     signal \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(waveform)\n\u001b[1;32m---> 22\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmfcc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Padding (no se procesa)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pad:\n",
      "File \u001b[1;32mc:\\Users\\sebas\\Desktop\\DL_señales\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\Desktop\\DL_señales\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\Desktop\\DL_señales\\.venv\\lib\\site-packages\\torchaudio\\transforms\\_transforms.py:699\u001b[0m, in \u001b[0;36mMFCC.forward\u001b[1;34m(self, waveform)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    692\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;124;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;124;03m        Tensor: specgram_mel_db of size (..., ``n_mfcc``, time).\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 699\u001b[0m     mel_specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMelSpectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_mels:\n\u001b[0;32m    701\u001b[0m         log_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\Desktop\\DL_señales\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\Desktop\\DL_señales\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\Desktop\\DL_señales\\.venv\\lib\\site-packages\\torchaudio\\transforms\\_transforms.py:619\u001b[0m, in \u001b[0;36mMelSpectrogram.forward\u001b[1;34m(self, waveform)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    612\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;124;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;124;03m        Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 619\u001b[0m     specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m     mel_specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmel_scale(specgram)\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mel_specgram\n",
      "File \u001b[1;32mc:\\Users\\sebas\\Desktop\\DL_señales\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\Desktop\\DL_señales\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\Desktop\\DL_señales\\.venv\\lib\\site-packages\\torchaudio\\transforms\\_transforms.py:110\u001b[0m, in \u001b[0;36mSpectrogram.forward\u001b[1;34m(self, waveform)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m        Fourier bins, and time is the number of window hops (n_frame).\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\Desktop\\DL_señales\\.venv\\lib\\site-packages\\torchaudio\\functional\\functional.py:126\u001b[0m, in \u001b[0;36mspectrogram\u001b[1;34m(waveform, pad, window, n_fft, hop_length, win_length, power, normalized, center, pad_mode, onesided, return_complex)\u001b[0m\n\u001b[0;32m    123\u001b[0m waveform \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# default values are consistent with librosa.core.spectrum._spectrogram\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m spec_f \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_length_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43monesided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# unpack batch\u001b[39;00m\n\u001b[0;32m    140\u001b[0m spec_f \u001b[38;5;241m=\u001b[39m spec_f\u001b[38;5;241m.\u001b[39mreshape(shape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m spec_f\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n",
      "File \u001b[1;32mc:\\Users\\sebas\\Desktop\\DL_señales\\.venv\\lib\\site-packages\\torch\\functional.py:666\u001b[0m, in \u001b[0;36mstft\u001b[1;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex)\u001b[0m\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(extended_shape), [pad, pad], pad_mode)\n\u001b[0;32m    665\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39msignal_dim:])\n\u001b[1;32m--> 666\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "e.train(epochs=100,patience=15,delta=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
